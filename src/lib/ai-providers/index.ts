import { OpenAIProvider, OpenAIConfig } from "./openai";
import { AnthropicProvider, AnthropicConfig } from "./anthropic";
import { GoogleProvider, GoogleConfig } from "./google";
import { HuggingFaceProvider, HuggingFaceConfig } from "./huggingface";

export interface AIResponse {
  content: string;
  confidence: number;
  tokens_used: number;
  model: string;
  provider: string;
}

export type AIProviderType = "openai" | "anthropic" | "google" | "huggingface";

export interface AIProviderConfig {
  openai?: OpenAIConfig;
  anthropic?: AnthropicConfig;
  google?: GoogleConfig;
  huggingface?: HuggingFaceConfig;
}

export class AIProviderManager {
  private providers: Map<AIProviderType, any> = new Map();
  private config: AIProviderConfig;

  constructor(config: AIProviderConfig) {
    this.config = config;
    this.initializeProviders();
  }

  private initializeProviders() {
    // Initialize OpenAI if configured
    if (this.config.openai?.apiKey) {
      this.providers.set("openai", new OpenAIProvider(this.config.openai));
    }

    // Initialize Anthropic if configured
    if (this.config.anthropic?.apiKey) {
      this.providers.set(
        "anthropic",
        new AnthropicProvider(this.config.anthropic)
      );
    }

    // Initialize Google if configured
    if (this.config.google?.apiKey) {
      this.providers.set("google", new GoogleProvider(this.config.google));
    }

    // Initialize HuggingFace if configured
    if (this.config.huggingface?.apiKey) {
      this.providers.set("huggingface", new HuggingFaceProvider(this.config.huggingface));
    }
  }

  async generateCompletion(
    provider: AIProviderType,
    prompt: string,
    model: string,
    options: {
      temperature?: number;
      maxTokens?: number;
      systemPrompt?: string;
      enableFallback?: boolean;
    } = {}
  ): Promise<AIResponse> {
    const { enableFallback = true, ...aiOptions } = options;

    // Define ordem de fallback baseada em VELOCIDADE (Haiku primeiro - OpenAI com problemas)
    // üîß REVERTENDO: HuggingFace como fallback at√© estar est√°vel
    const fallbackOrder: AIProviderType[] = ["anthropic", "google", "huggingface", "openai"];

    // Se um provedor espec√≠fico foi solicitado, tentar ele primeiro
    const providersToTry = provider
      ? [provider, ...fallbackOrder.filter((p) => p !== provider)]
      : fallbackOrder;

    // Use provider-specific default models (otimizado para VELOCIDADE)
    const defaultModels = {
      openai: "gpt-4o-mini", // MUITO R√ÅPIDO - Melhor custo-benef√≠cio
      anthropic: "claude-3-5-haiku-20241022", // MUITO R√ÅPIDO - Excelente para an√°lises
      google: "gemini-1.5-flash", // R√ÅPIDO - Mais r√°pido que 2.5-flash
      huggingface: "meta-llama/Llama-3.2-3B-Instruct", // MUITO R√ÅPIDO - Open Source
    };

    let lastError: Error | null = null;

    for (const currentProvider of providersToTry) {
      const providerInstance = this.providers.get(currentProvider);

      if (!providerInstance) {
        console.warn(
          `üîÑ Provider '${currentProvider}' not configured, trying next...`
        );
        continue;
      }

      try {
        console.log(`ü§ñ Attempting AI call with ${currentProvider}...`);

        // Usar modelo compat√≠vel com o provider atual
        const modelToUse = this.getCompatibleModel(
          model || "gpt-4",
          currentProvider
        );
        
        // üîß SISTEMA UNIVERSAL JSON - FOR√áA RESPOSTA EM JSON SEMPRE
        const universalSystemPrompt = `Voc√™ √© um assistente especializado em an√°lise de documentos. 
IMPORTANTE: Responda SEMPRE em formato JSON v√°lido, seguindo esta estrutura:

{
  "metadata": {
    "tipo_documento": "tipo detectado automaticamente",
    "titulo_relatorio": "T√≠tulo adequado para o tipo de documento", 
    "data_analise": "${new Date().toLocaleDateString('pt-BR')}",
    "tipo_analise": "Tipo de an√°lise realizada"
  },
  "analise_payload": {
    "resumo_executivo": "Resumo conciso da an√°lise",
    "dados_principais": {
      "informacao_extraida": "valores extra√≠dos do documento"
    },
    "pontos_principais": [
      "Primeiro ponto importante",
      "Segundo ponto importante"
    ],
    "recomendacoes": [
      "Primeira recomenda√ß√£o",
      "Segunda recomenda√ß√£o"
    ],
    "conclusao": "Conclus√£o final da an√°lise"
  }
}

REGRAS OBRIGAT√ìRIAS:
- Responda APENAS JSON v√°lido.
- N√£o use markdown, asteriscos ou formata√ß√£o.
- Dentro de 'analise_payload', voc√™ tem a flexibilidade de adicionar, remover ou renomear campos para melhor representar os dados do documento analisado.
- Use "N√£o informado" se dados n√£o dispon√≠veis.

${aiOptions.systemPrompt || this.getDefaultSystemPrompt(currentProvider)}`;

        console.log('üéØ [AIProviderManager] Using Universal JSON System');
        
        const systemPrompt = universalSystemPrompt;

        console.log(
          `üìã Using model: ${modelToUse} for provider: ${currentProvider} (requested: ${
            model || "default"
          })`
        );

        const result = await providerInstance.generateCompletion(
          prompt,
          modelToUse,
          {
            ...aiOptions,
            systemPrompt,
          }
        );

        console.log(`‚úÖ AI call successful with ${currentProvider}`);
        return {
          ...result,
          provider: currentProvider, // Garantir que o provider correto seja retornado
        };
      } catch (error) {
        lastError = error as Error;
        console.error(`‚ùå Provider ${currentProvider} failed:`, error);

        // Se n√£o √© para usar fallback ou √© o √∫ltimo provider, lan√ßar erro
        if (
          !enableFallback ||
          currentProvider === providersToTry[providersToTry.length - 1]
        ) {
          break;
        }

        console.log(`üîÑ Trying fallback provider...`);
        continue;
      }
    }

    // Se chegou aqui, todos os providers falharam
    const availableProviders = this.getAvailableProviders();
    throw new Error(
      `All AI providers failed. Last error: ${lastError?.message}. ` +
        `Available providers: ${availableProviders.join(", ")}. ` +
        `Check your API keys and network connection.`
    );
  }

  async generateStreamCompletion(
    provider: AIProviderType,
    prompt: string,
    model: string,
    options: {
      temperature?: number;
      maxTokens?: number;
      systemPrompt?: string;
      onChunk?: (chunk: string) => void;
      enableFallback?: boolean;
    } = {}
  ): Promise<AIResponse> {
    const { enableFallback = true, ...aiOptions } = options;

    // Define ordem de fallback para streaming (Haiku primeiro - OpenAI com problemas)
    const fallbackOrder: AIProviderType[] = ["anthropic", "google", "openai"];
    const providersToTry = provider
      ? [provider, ...fallbackOrder.filter((p) => p !== provider)]
      : fallbackOrder;

    const defaultModels = {
      openai: "gpt-4o-mini",
      anthropic: "claude-3-5-haiku-20241022",
      google: "gemini-1.5-flash",
    };

    let lastError: Error | null = null;

    for (const currentProvider of providersToTry) {
      const providerInstance = this.providers.get(currentProvider);

      if (!providerInstance) {
        console.warn(
          `üîÑ Stream provider '${currentProvider}' not configured, trying next...`
        );
        continue;
      }

      try {
        console.log(`üåä Attempting streaming with ${currentProvider}...`);

        // Se n√£o suporta streaming, usar completion normal
        if (!providerInstance.generateStreamCompletion) {
          console.log(
            `üìù ${currentProvider} doesn't support streaming, using regular completion`
          );
          return await this.generateCompletion(
            currentProvider,
            prompt,
            model,
            aiOptions
          );
        }

        // Usar modelo compat√≠vel com o provider atual
        const modelToUse = this.getCompatibleModel(
          model || "gpt-4",
          currentProvider
        );
        const systemPrompt =
          aiOptions.systemPrompt ||
          this.getDefaultSystemPrompt(currentProvider);

        console.log(
          `üìã Streaming with model: ${modelToUse} for provider: ${currentProvider} (requested: ${
            model || "default"
          })`
        );

        const result = await providerInstance.generateStreamCompletion(
          prompt,
          modelToUse,
          {
            ...aiOptions,
            systemPrompt,
          }
        );

        console.log(`‚úÖ Streaming successful with ${currentProvider}`);
        return {
          ...result,
          provider: currentProvider,
        };
      } catch (error) {
        lastError = error as Error;
        console.error(`‚ùå Stream provider ${currentProvider} failed:`, error);

        if (
          !enableFallback ||
          currentProvider === providersToTry[providersToTry.length - 1]
        ) {
          break;
        }

        console.log(`üîÑ Trying next streaming provider...`);
        continue;
      }
    }

    // Se chegou aqui, todos falharam
    throw new Error(
      `All streaming providers failed. Last error: ${lastError?.message}. ` +
        `Falling back to regular completion...`
    );
  }

  /**
   * M√©todo inteligente que tenta automaticamente o melhor provider dispon√≠vel
   */
  async generateCompletionWithAutoFallback(
    prompt: string,
    options: {
      preferredProvider?: AIProviderType;
      model?: string;
      temperature?: number;
      maxTokens?: number;
      systemPrompt?: string;
      prioritizeCost?: boolean;
    } = {}
  ): Promise<
    AIResponse & { actualProvider: AIProviderType; fallbackUsed: boolean }
  > {
    const { preferredProvider, prioritizeCost = true, ...aiOptions } = options;

    // Ordem baseada em velocidade e custo-benef√≠cio (Haiku primeiro - OpenAI com problemas)
    let providerOrder: AIProviderType[] = prioritizeCost
      ? ["anthropic", "google", "openai"] // Haiku + custo otimizado
      : ["anthropic", "google", "openai"]; // Por velocidade/qualidade

    // Se tem prefer√™ncia, colocar primeiro
    if (preferredProvider && this.isProviderAvailable(preferredProvider)) {
      providerOrder = [
        preferredProvider,
        ...providerOrder.filter((p) => p !== preferredProvider),
      ];
    }

    let fallbackUsed = false;

    for (let i = 0; i < providerOrder.length; i++) {
      const currentProvider = providerOrder[i];

      if (!this.isProviderAvailable(currentProvider)) {
        continue;
      }

      try {
        if (i > 0) fallbackUsed = true;

        const result = await this.generateCompletion(
          currentProvider,
          prompt,
          "", // Deixar vazio para usar modelo padr√£o do provider
          { ...aiOptions, enableFallback: false } // Desabilitar fallback interno para controle manual
        );

        return {
          ...result,
          actualProvider: currentProvider,
          fallbackUsed,
        };
      } catch (error) {
        console.error(
          `Provider ${currentProvider} failed, trying next...`,
          error
        );
        continue;
      }
    }

    throw new Error(
      "All AI providers are unavailable or failed. Please check your configuration."
    );
  }

  /**
   * Mapeia modelos entre providers para compatibilidade
   */
  private getCompatibleModel(
    requestedModel: string,
    targetProvider: AIProviderType
  ): string {
    const modelMapping: Record<string, Record<AIProviderType, string>> = {
      // Modelos GPT-4 equivalentes (OTIMIZADO PARA VELOCIDADE)
      "gpt-4": {
        openai: "gpt-4o-mini",
        anthropic: "claude-3-5-haiku-20241022",
        google: "gemini-1.5-flash", // ‚ö° Mais r√°pido que 2.5-flash
        huggingface: "meta-llama/Llama-3.2-3B-Instruct", // ‚ö° R√°pido e eficiente
      },
      "gpt-4o-mini": {
        openai: "gpt-4o-mini",
        anthropic: "claude-3-5-haiku-20241022",
        google: "gemini-1.5-flash", // ‚ö° Mais r√°pido que 2.5-flash
        huggingface: "meta-llama/Llama-3.2-3B-Instruct", // ‚ö° R√°pido e eficiente
      },
      // Modelos Claude equivalentes
      "claude-3-sonnet": {
        openai: "gpt-4o-mini",
        anthropic: "claude-3-5-haiku-20241022",
        google: "gemini-1.5-flash", // ‚ö° Mais r√°pido que 2.5-flash
        huggingface: "meta-llama/Llama-3.2-3B-Instruct", // ‚ö° R√°pido e eficiente
      },
      // Modelos Gemini equivalentes
      "gemini-pro": {
        openai: "gpt-4o-mini",
        anthropic: "claude-3-5-haiku-20241022",
        google: "gemini-1.5-flash", // ‚ö° Mais r√°pido que 2.5-flash
        huggingface: "meta-llama/Llama-3.2-3B-Instruct", // ‚ö° R√°pido e eficiente
      },
      "gemini-2.5-flash": {
        openai: "gpt-4o-mini",
        anthropic: "claude-3-5-haiku-20241022",
        google: "gemini-1.5-flash", // ‚ö° Upgrade para vers√£o mais r√°pida
        huggingface: "meta-llama/Llama-3.2-3B-Instruct", // ‚ö° R√°pido e eficiente
      },
    };

    // Se h√° mapeamento espec√≠fico, usar ele
    if (modelMapping[requestedModel]) {
      return modelMapping[requestedModel][targetProvider];
    }

    // Sen√£o, usar modelo padr√£o do provider (OTIMIZADO PARA VELOCIDADE)
    const defaultModels = {
      openai: "gpt-4o-mini",
      anthropic: "claude-3-5-haiku-20241022",
      google: "gemini-1.5-flash", // ‚ö° Mais r√°pido que 2.5-flash
      huggingface: "meta-llama/Llama-3.2-3B-Instruct", // ‚ö° R√°pido e eficiente
    };

    return defaultModels[targetProvider];
  }

  private getDefaultSystemPrompt(provider: AIProviderType): string {
    const basePrompt = `Voc√™ √© um assistente especializado em an√°lise de documentos e processos de Recursos Humanos (RH).

Suas especialidades incluem:
- An√°lise de contratos de trabalho e conformidade com a CLT
- Processamento de documentos de RH (curr√≠culos, avalia√ß√µes, onboarding)
- Gera√ß√£o de relat√≥rios profissionais em HTML
- Valida√ß√£o de dados e documentos trabalhistas
- Suporte a processos de recrutamento e sele√ß√£o

Sempre forne√ßa respostas precisas, profissionais e em conformidade com a legisla√ß√£o trabalhista brasileira.`;

    // Provider-specific adjustments
    switch (provider) {
      case "anthropic":
        return `${basePrompt}\n\nSeja detalhado e estruturado em suas an√°lises.`;
      case "openai":
        return `${basePrompt}\n\nForne√ßa respostas claras e organizadas.`;
      case "google":
        return `${basePrompt}\n\nSeja preciso e objetivo em suas respostas.`;
      default:
        return basePrompt;
    }
  }

  getAvailableProviders(): AIProviderType[] {
    return Array.from(this.providers.keys());
  }

  isProviderAvailable(provider: AIProviderType): boolean {
    return this.providers.has(provider);
  }

  async testProvider(provider: AIProviderType): Promise<boolean> {
    try {
      const result = await this.generateCompletion(
        provider,
        'Teste de conectividade. Responda apenas "OK".',
        "",
        { maxTokens: 10 }
      );
      return result.content.toLowerCase().includes("ok");
    } catch (error) {
      console.error(`Provider ${provider} test failed:`, error);
      return false;
    }
  }
}

// Export individual providers for direct use if needed
export { OpenAIProvider, AnthropicProvider, GoogleProvider };
export type { OpenAIConfig, AnthropicConfig, GoogleConfig };
